{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/prajwal/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/prajwal/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, Sequence, Dict, Union, Optional\n",
    "import numpy as np\n",
    "import math\n",
    "import collections\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal/workfolder/SAC-X-DT/.conda/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/prajwal/workfolder/SAC-X-DT/.conda/lib/python3.10/site-packages/gym/envs/mujoco/mujoco_env.py:191: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n",
      "load datafile: 100%|██████████| 11/11 [00:00<00:00, 34.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "dataset_name1 = \"hopper-medium-replay-v2\"\n",
    "dataset1, env_info = get_dataset(dataset_name1)\n",
    "env_name = env_info['env_name']\n",
    "state_dim = env_info['state_dim']\n",
    "act_dim = env_info['action_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 21/21 [00:00<00:00, 28.74it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name2 = \"hopper-expert-v2\"\n",
    "dataset2, _ = get_dataset(dataset_name2)\n",
    "\n",
    "dataset1_obs = np.concatenate([dataset1['infos/qpos'], dataset1['infos/qvel']], axis=1)\n",
    "dataset2_obs = np.concatenate([dataset2['infos/qpos'], dataset2['infos/qvel']], axis=1)\n",
    "observations = np.concatenate((dataset1_obs, dataset2_obs), axis=0)\n",
    "actions = np.concatenate((dataset1['actions'], dataset2['actions']), axis=0)\n",
    "next_observations = np.concatenate((dataset1['next_observations'], dataset2['next_observations']), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_observations = np.roll(observations, -1, axis=0)\n",
    "mask = ~np.concatenate((dataset1['terminals']+dataset1['timeouts'], dataset2['terminals']+dataset2['timeouts']), axis=0)\n",
    "observations = np.array(observations[mask], dtype=np.float32)\n",
    "actions = np.array(actions[mask], dtype=np.float32)\n",
    "next_observations = np.array(next_observations[mask], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs:  array([-0.0024,  1.2498,  0.0041, ...,  0.0044,  0.0015,  0.0023],\n",
      "      dtype=float32)\n",
      "Action:  array([-0.438 ,  0.3708, -0.9334], dtype=float32)\n",
      "Next Obs:  array([-0.0027,  1.2496,  0.0018, ..., -0.6151, -0.0082, -1.3391],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub.utils import IGNORE_GIT_FOLDER_PATTERNS\n",
    "obs = observations[0]\n",
    "act = actions[0]\n",
    "next_obs = next_observations[0]\n",
    "with np.printoptions(precision=4, suppress=True, threshold=5):\n",
    "    print(\"Obs: \", repr(obs))\n",
    "    print(\"Action: \", repr(act))\n",
    "    print(\"Next Obs: \", repr(next_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_indices(\n",
    "        episode_ends:np.ndarray, sequence_length:int,\n",
    "        pad_before: int=0, pad_after: int=0):\n",
    "    indices = list()\n",
    "    for i in range(len(episode_ends)):\n",
    "        start_idx = 0\n",
    "        if i > 0:\n",
    "            start_idx = episode_ends[i-1]\n",
    "        end_idx = episode_ends[i]\n",
    "        episode_length = end_idx - start_idx\n",
    "\n",
    "        min_start = -pad_before\n",
    "        max_start = episode_length - sequence_length + pad_after\n",
    "\n",
    "        # range stops one idx before end\n",
    "        for idx in range(min_start, max_start+1):\n",
    "            buffer_start_idx = max(idx, 0) + start_idx\n",
    "            buffer_end_idx = min(idx+sequence_length, episode_length) + start_idx\n",
    "            start_offset = buffer_start_idx - (idx+start_idx)\n",
    "            end_offset = (idx+sequence_length+start_idx) - buffer_end_idx\n",
    "            sample_start_idx = 0 + start_offset\n",
    "            sample_end_idx = sequence_length - end_offset\n",
    "            indices.append([\n",
    "                buffer_start_idx, buffer_end_idx,\n",
    "                sample_start_idx, sample_end_idx])\n",
    "    indices = np.array(indices)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def sample_sequence(train_data, sequence_length,\n",
    "                    buffer_start_idx, buffer_end_idx,\n",
    "                    sample_start_idx, sample_end_idx):\n",
    "    result = dict()\n",
    "    for key, input_arr in train_data.items():\n",
    "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
    "        data = sample\n",
    "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
    "            data = np.zeros(\n",
    "                shape=(sequence_length,) + input_arr.shape[1:],\n",
    "                dtype=input_arr.dtype)\n",
    "            if sample_start_idx > 0:\n",
    "                data[:sample_start_idx] = sample[0]\n",
    "            if sample_end_idx < sequence_length:\n",
    "                data[sample_end_idx:] = sample[-1]\n",
    "            data[sample_start_idx:sample_end_idx] = sample\n",
    "        result[key] = data\n",
    "    return result\n",
    "\n",
    "# normalize data\n",
    "def get_data_stats(data):\n",
    "    data = data.reshape(-1,data.shape[-1])\n",
    "    stats = {\n",
    "        'min': np.min(data, axis=0),\n",
    "        'max': np.max(data, axis=0)\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "def normalize_data(data, stats):\n",
    "    # nomalize to [0,1]\n",
    "    ndata = (data - stats['min']) / (stats['max'] - stats['min'])\n",
    "    # normalize to [-1, 1]\n",
    "    ndata = ndata * 2 - 1\n",
    "    return ndata\n",
    "\n",
    "def unnormalize_data(ndata, stats):\n",
    "    ndata = (ndata + 1) / 2\n",
    "    data = ndata * (stats['max'] - stats['min']) + stats['min']\n",
    "    return data\n",
    "\n",
    "# dataset\n",
    "class DynamicsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_root, pred_horizon=1, obs_horizon=1, next_obs_horizon=1):\n",
    "\n",
    "\n",
    "        # All demonstration episodes are concatinated in the first dimension N\n",
    "        train_data = {\n",
    "            # (N, action_dim)\n",
    "            'state_action': dataset_root['state_action'],\n",
    "            # (N, obs_dim)\n",
    "            'next_state': dataset_root['next_state'],\n",
    "        }\n",
    "        # Marks one-past the last index for each episode\n",
    "        episode_ends = dataset_root['episode_ends']\n",
    "\n",
    "        # compute start and end of each state-action sequence\n",
    "        # also handles padding\n",
    "        indices = create_sample_indices(\n",
    "            episode_ends=episode_ends,\n",
    "            sequence_length=pred_horizon,\n",
    "            # add padding such that each timestep in the dataset are seen\n",
    "            pad_before=obs_horizon-1,\n",
    "            pad_after=next_obs_horizon-1)\n",
    "\n",
    "        # compute statistics and normalized data to [-1,1]\n",
    "        stats = dict()\n",
    "        normalized_train_data = dict()\n",
    "        for key, data in train_data.items():\n",
    "            stats[key] = get_data_stats(data)\n",
    "            normalized_train_data[key] = normalize_data(data, stats[key])\n",
    "\n",
    "        self.indices = indices\n",
    "        self.stats = stats\n",
    "        self.normalized_train_data = normalized_train_data\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.next_obs_horizon = next_obs_horizon\n",
    "        self.obs_horizon = obs_horizon\n",
    "\n",
    "    def __len__(self):\n",
    "        # all possible segments of the dataset\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the start/end indices for this datapoint\n",
    "        buffer_start_idx, buffer_end_idx, \\\n",
    "            sample_start_idx, sample_end_idx = self.indices[idx]\n",
    "\n",
    "        # get nomralized data using these indices\n",
    "        nsample = sample_sequence(\n",
    "            train_data=self.normalized_train_data,\n",
    "            sequence_length=self.pred_horizon,\n",
    "            buffer_start_idx=buffer_start_idx,\n",
    "            buffer_end_idx=buffer_end_idx,\n",
    "            sample_start_idx=sample_start_idx,\n",
    "            sample_end_idx=sample_end_idx\n",
    "        )\n",
    "\n",
    "        # discard unused observations\n",
    "        nsample['state_action'] = nsample['state_action'][:self.obs_horizon,:]\n",
    "        return nsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = {\n",
    "    'state_action': np.concatenate((observations, actions), axis=1),\n",
    "    'next_state': next_observations,\n",
    "    'episode_ends': np.array([len(observations)])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats: \n",
      "state_action:\n",
      "Min:  [ -5.0355263    0.70533     -0.19956256  -1.7470077   -1.9270817\n",
      "  -0.9721309   -2.2943766   -5.580463    -5.682034   -10.\n",
      " -10.         -10.          -0.9999998   -0.99999475  -1.        ]\n",
      "Max:  [22.013454    1.8055555   0.19973862  0.05772436  0.12416191  0.97480917\n",
      "  5.6330166   3.3520665   7.750577   10.         10.         10.\n",
      "  1.          1.          1.        ]\n",
      "next_state:\n",
      "Min:  [ -5.042045     0.7000183   -0.19996448  -1.7813634   -1.9897504\n",
      "  -0.9721309   -2.2943766   -5.959067    -5.8681846  -10.\n",
      " -10.         -10.        ]\n",
      "Max:  [22.047226    1.8055555   0.19989353  0.05772436  0.12416191  0.97480917\n",
      "  5.6330166   3.3520665   7.9372845  10.         10.         10.        ]\n"
     ]
    }
   ],
   "source": [
    "hopper_dataset = DynamicsDataset(dataset_root)\n",
    "stats = hopper_dataset.stats\n",
    "print(\"Stats: \")\n",
    "for key, stat in stats.items():\n",
    "    print(key+\":\")\n",
    "    print(\"Min: \", stat['min'])\n",
    "    print(\"Max: \", stat['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: \n",
      "state_action torch.Size([256, 1, 15])\n",
      "next_state torch.Size([256, 1, 12])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(hopper_dataset, batch_size=256,\n",
    "                        num_workers=1, shuffle=True,\n",
    "                        pin_memory=True, persistent_workers=True)\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "print(\"Batch: \")\n",
    "for key, val in batch.items():\n",
    "    print(key, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 3, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dim = observations.shape[1]\n",
    "act_dim = actions.shape[1]\n",
    "next_state_dim = next_observations.shape[1]\n",
    "state_dim, act_dim, next_state_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### **Network**\n",
    "#@markdown\n",
    "#@markdown Defines a 1D UNet architecture `ConditionalUnet1D`\n",
    "#@markdown as the noies prediction network\n",
    "#@markdown\n",
    "#@markdown Components\n",
    "#@markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k\n",
    "#@markdown - `Downsample1d` Strided convolution to reduce temporal resolution\n",
    "#@markdown - `Upsample1d` Transposed convolution to increase temporal resolution\n",
    "#@markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish\n",
    "#@markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \\\n",
    "#@markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection.\n",
    "#@markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class Downsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Upsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Conv1dBlock(nn.Module):\n",
    "    '''\n",
    "        Conv1d --> GroupNorm --> Mish\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
    "            nn.GroupNorm(n_groups, out_channels),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ConditionalResidualBlock1D(nn.Module):\n",
    "    def __init__(self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            cond_dim,\n",
    "            kernel_size=3,\n",
    "            n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "        ])\n",
    "\n",
    "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
    "        # predicts per-channel scale and bias\n",
    "        cond_channels = out_channels * 2\n",
    "        self.out_channels = out_channels\n",
    "        self.cond_encoder = nn.Sequential(\n",
    "            nn.Mish(),\n",
    "            nn.Linear(cond_dim, cond_channels),\n",
    "            nn.Unflatten(-1, (-1, 1))\n",
    "        )\n",
    "\n",
    "        # make sure dimensions compatible\n",
    "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
    "            if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        '''\n",
    "            x : [ batch_size x in_channels x horizon ]\n",
    "            cond : [ batch_size x cond_dim]\n",
    "\n",
    "            returns:\n",
    "            out : [ batch_size x out_channels x horizon ]\n",
    "        '''\n",
    "        out = self.blocks[0](x)\n",
    "        embed = self.cond_encoder(cond)\n",
    "\n",
    "        embed = embed.reshape(\n",
    "            embed.shape[0], 2, self.out_channels, 1)\n",
    "        scale = embed[:,0,...]\n",
    "        bias = embed[:,1,...]\n",
    "        out = scale * out + bias\n",
    "\n",
    "        out = self.blocks[1](out)\n",
    "        out = out + self.residual_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConditionalUnet1D(nn.Module):\n",
    "    def __init__(self,\n",
    "        input_dim,\n",
    "        global_cond_dim,\n",
    "        diffusion_step_embed_dim=256,\n",
    "        down_dims=[256,512,1024],\n",
    "        kernel_size=5,\n",
    "        n_groups=8\n",
    "        ):\n",
    "        \"\"\"\n",
    "        input_dim: Dim of actions.\n",
    "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
    "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
    "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
    "        down_dims: Channel size for each UNet level.\n",
    "          The length of this array determines numebr of levels.\n",
    "        kernel_size: Conv kernel size\n",
    "        n_groups: Number of groups for GroupNorm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        all_dims = [input_dim] + list(down_dims)\n",
    "        start_dim = down_dims[0]\n",
    "\n",
    "        dsed = diffusion_step_embed_dim\n",
    "        diffusion_step_encoder = nn.Sequential(\n",
    "            SinusoidalPosEmb(dsed),\n",
    "            nn.Linear(dsed, dsed * 4),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(dsed * 4, dsed),\n",
    "        )\n",
    "        cond_dim = dsed + global_cond_dim\n",
    "\n",
    "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
    "        mid_dim = all_dims[-1]\n",
    "        self.mid_modules = nn.ModuleList([\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        down_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            down_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        up_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            up_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        final_conv = nn.Sequential(\n",
    "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
    "            nn.Conv1d(start_dim, input_dim, 1),\n",
    "        )\n",
    "\n",
    "        self.diffusion_step_encoder = diffusion_step_encoder\n",
    "        self.up_modules = up_modules\n",
    "        self.down_modules = down_modules\n",
    "        self.final_conv = final_conv\n",
    "\n",
    "        print(\"number of parameters: {:e}\".format(\n",
    "            sum(p.numel() for p in self.parameters()))\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "            sample: torch.Tensor,\n",
    "            timestep: Union[torch.Tensor, float, int],\n",
    "            global_cond=None):\n",
    "        \"\"\"\n",
    "        x: (B,T,input_dim)\n",
    "        timestep: (B,) or int, diffusion step\n",
    "        global_cond: (B,global_cond_dim)\n",
    "        output: (B,T,input_dim)\n",
    "        \"\"\"\n",
    "        # (B,T,C)\n",
    "        sample = sample.moveaxis(-1,-2)\n",
    "        # (B,C,T)\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "        global_feature = self.diffusion_step_encoder(timesteps)\n",
    "\n",
    "        if global_cond is not None:\n",
    "            global_feature = torch.cat([\n",
    "                global_feature, global_cond\n",
    "            ], axis=-1)\n",
    "\n",
    "        x = sample\n",
    "        h = []\n",
    "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        for mid_module in self.mid_modules:\n",
    "            x = mid_module(x, global_feature)\n",
    "\n",
    "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
    "            # print(x.shape, h[-1].shape)\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # (B,C,T)\n",
    "        x = x.moveaxis(-1,-2)\n",
    "        # (B,T,C)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = state_dim # 12 for hopper (noise prediction)\n",
    "obs_dim = state_dim + act_dim # 12 + 3 for hopper (state + action)\n",
    "obs_horizon = 1 # 1 for markov dynamics\n",
    "global_cond_dim = obs_horizon * obs_dim # 15 for film conditioning on previous state-action\n",
    "pred_horizon = 1 # 1 for next state prediction\n",
    "\n",
    "# lets use pred_horizon = 16 for now\n",
    "# pred_horizon = 1 throws an error\n",
    "pred_horizon = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 6.544283e+07\n",
      "Noised Next State:  torch.Size([1, 16, 12])\n"
     ]
    }
   ],
   "source": [
    "noise_pred_net = ConditionalUnet1D(\n",
    "    input_dim=input_dim,\n",
    "    global_cond_dim=global_cond_dim,\n",
    ")\n",
    "# conditional unet is a noise prediction network, initialized with input_dim and global_cond_dim\n",
    "# input_dim is the dimension of the input and output of the network\n",
    "# global_cond_dim is the dimension of the global conditioning applied with FiLM in addition to diffusion step embedding\n",
    "\n",
    "# example inputs\n",
    "noised_next_state = torch.randn((1, pred_horizon, input_dim))\n",
    "obs = torch.zeros((1, obs_horizon, obs_dim))\n",
    "diffusion_iter = torch.zeros((1,))\n",
    "\n",
    "\n",
    "# the noise prediction network\n",
    "# takes noisy action, diffusion iteration and observation as input\n",
    "# predicts the noise added to action\n",
    "print(\"Noised Next State: \", noised_next_state.shape)\n",
    "noise = noise_pred_net(\n",
    "    sample=noised_next_state,\n",
    "    timestep=diffusion_iter,\n",
    "    global_cond=obs.flatten(start_dim=1))\n",
    "\n",
    "# illustration of removing noise\n",
    "# the actual noise removal is performed by NoiseScheduler\n",
    "# and is dependent on the diffusion noise schedule\n",
    "denoised_action = noised_next_state - noise\n",
    "\n",
    "# DDPMScheduler with 100 diffusion iterations\n",
    "num_diffusion_iters = 100\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    # clip output to [-1,1] to improve stability\n",
    "    clip_sample=True,\n",
    "    # the network predicts noise (instead of denoised action)\n",
    "    prediction_type='epsilon'\n",
    ")\n",
    "\n",
    "# device transfer\n",
    "device = torch.device('cuda')\n",
    "_ = noise_pred_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 100 \n",
    "\n",
    "# Exponential Moving Average\n",
    "# accelerates training and improves stability\n",
    "# holds a copy of the model weights\n",
    "ema = EMAModel(\n",
    "    parameters=noise_pred_net.parameters(),\n",
    "    power=0.75)\n",
    "\n",
    "# Standard ADAM optimizer\n",
    "# Note that EMA parametesr are not optimized\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=noise_pred_net.parameters(),\n",
    "    lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# Cosine LR schedule with linear warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
    "    # epoch loop\n",
    "    for epoch_idx in tglobal:\n",
    "        epoch_loss = list()\n",
    "        # batch loop\n",
    "        with tqdm(dataloader, desc='Batch', leave=False) as tepoch:\n",
    "            for nbatch in tepoch:\n",
    "                # data normalized in dataset\n",
    "                # device transfer\n",
    "                nobs = nbatch['state_action'].to(device)\n",
    "                nnextobs = nbatch['next_state'].to(device)\n",
    "                B = nobs.shape[0]\n",
    "\n",
    "                # observation as FiLM conditioning\n",
    "                # (B, obs_horizon, obs_dim)\n",
    "                obs_cond = nobs[:,:obs_horizon,:]\n",
    "                # (B, obs_horizon * obs_dim)\n",
    "                obs_cond = obs_cond.flatten(start_dim=1)\n",
    "\n",
    "                # sample noise to add to nextobs\n",
    "                noise_shape = nnextobs.shape\n",
    "                # nnextobs shape is (B, pred_horizon, input_dim)\n",
    "                # pred_horizon on dataset is 1, as we are predicting the next state only\n",
    "                # the networks throws an error if pred_horizon is < 16\n",
    "                # change noise_shape to (B, 16, input_dim)\n",
    "                noise_shape = (noise_shape[0], pred_horizon, noise_shape[-1])\n",
    "                noise = torch.randn(noise_shape, device=device)\n",
    "\n",
    "                # sample a diffusion iteration for each data point\n",
    "                timesteps = torch.randint(\n",
    "                    0, noise_scheduler.config.num_train_timesteps,\n",
    "                    (B,), device=device\n",
    "                ).long()\n",
    "\n",
    "                # add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_nextobs = noise_scheduler.add_noise(\n",
    "                    nnextobs, noise, timesteps)\n",
    "                \n",
    "                # nnextobs (B, pred_horizon_dataset, input_dim), pred_horizon_dataset = 1\n",
    "                # noise (B, pred_horizon, input_dim), pred_horizon = 16 required to run the network\n",
    "                # noisy_nextobs <- nnextobs + noise at each diffusion iteration\n",
    "                # noisy_nextobs (B, pred_horizon, input_dim), pred_horizon = 16\n",
    "                \n",
    "\n",
    "                # predict the noise residual\n",
    "                noise_pred = noise_pred_net(\n",
    "                    noisy_nextobs, timesteps, global_cond=obs_cond)\n",
    "                \n",
    "                # Based on the noisy nextobs, the network predicts the noise added to the nextobs\n",
    "                # conditioned on the observation\n",
    "\n",
    "                # L2 loss between predicted noise and actual noise\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "\n",
    "                # optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # step lr scheduler every batch\n",
    "                # this is different from standard pytorch behavior\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                # update Exponential Moving Average of the model weights\n",
    "                ema.step(noise_pred_net.parameters())\n",
    "\n",
    "                # logging\n",
    "                loss_cpu = loss.item()\n",
    "                epoch_loss.append(loss_cpu)\n",
    "                tepoch.set_postfix(loss=loss_cpu)\n",
    "        if (epoch_idx+1) % 50 == 0:\n",
    "            torch.save(noise_pred_net.state_dict(), f'./DiffusionDynamicsModels/ddpm_hopper_dynamics_{epoch_idx+1}.pth')\n",
    "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
    "\n",
    "# Weights of the EMA model\n",
    "# is used for inference\n",
    "ema_noise_pred_net = noise_pred_net\n",
    "ema.copy_to(ema_noise_pred_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "ema_noise_pred_net = noise_pred_net\n",
    "ema_noise_pred_net.load_state_dict(torch.load('./DiffusionDynamicsModels/ddpm_hopper_dynamics_100.pth'))\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dynamics(\n",
    "    model: nn.Module,\n",
    "    obs,\n",
    "    act,\n",
    "    obs_horizon: int = 1,\n",
    "    device: str = 'cuda'\n",
    "):\n",
    "    \n",
    "    # the function assumes a normalized datapoint \n",
    "    # and returns an unnormalized prediction\n",
    "\n",
    "    # nobs is (T, obs_dim + act_dim) \n",
    "    \n",
    "    if not isinstance(obs, torch.Tensor):\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            nobs = np.concatenate((obs, act), axis=-1)\n",
    "        else:\n",
    "            print(\"Use torch tensors or numpy arrays as input!\")\n",
    "        nobs = torch.tensor(nobs, dtype=torch.float32, device=device).detach()\n",
    "    else:\n",
    "        nobs = torch.cat((obs, act), dim=-1)\n",
    "        nobs = nobs.to(device).to(torch.float32).clone().detach()\n",
    "\n",
    "    # add a code to incorporate the obs_horizon\n",
    "    obs_horizon = int(obs_horizon)\n",
    "    pred_horizon = 16 # for now, debug later\n",
    "\n",
    "    B = obs.shape[0]\n",
    "    obs_dim = obs.shape[-1]\n",
    "\n",
    "    # infer the next state\n",
    "    with torch.no_grad():\n",
    "        obs_cond = nobs.clone().detach()\n",
    "        noisy_nextobs = torch.randn((B, pred_horizon, obs_dim), device=device)\n",
    "\n",
    "        noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "        \n",
    "        for k in noise_scheduler.timesteps:\n",
    "            noise = model(\n",
    "                sample = noisy_nextobs,\n",
    "                timestep = k,\n",
    "                global_cond = obs_cond\n",
    "            )\n",
    "            noisy_nextobs = noise_scheduler.step(\n",
    "                model_output=noise,\n",
    "                timestep=k,\n",
    "                sample=noisy_nextobs\n",
    "            ).prev_sample\n",
    "\n",
    "    noisy_nextobs = noisy_nextobs.cpu().numpy()\n",
    "    noisy_nextobs = noisy_nextobs[:,0,:]\n",
    "    return noisy_nextobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15])\n",
      "Predicted Next State:  [[-5.7977831e-01 -5.5535835e-01 -9.0648212e-02  7.9119086e-02\n",
      "   8.8451844e-01  7.4196362e-01 -1.5411229e-01  1.2442957e-01\n",
      "  -1.5479720e-01 -1.4843488e-01  5.6755950e-04  6.2835328e-03]]\n",
      "Actual Next State:  tensor([[-5.7969e-01, -5.5526e-01, -9.0659e-02,  7.9296e-02,  8.8481e-01,\n",
      "          7.4183e-01, -1.5465e-01,  1.2446e-01, -1.5484e-01, -1.4836e-01,\n",
      "          6.2013e-04,  6.0327e-03]])\n"
     ]
    }
   ],
   "source": [
    "# use the model to predict the next state\n",
    "sample_data = next(iter(dataloader))\n",
    "obs = sample_data['state_action'][0]\n",
    "act = obs[0:1, state_dim:]\n",
    "obs = obs[0:1, :state_dim]\n",
    "next_obs = sample_data['next_state'][0]\n",
    "pred = predict_dynamics(\n",
    "    model=ema_noise_pred_net,\n",
    "    obs=obs,\n",
    "    act=act,\n",
    "    obs_horizon=1,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "print(\"Predicted Next State: \", pred)\n",
    "print(\"Actual Next State: \", next_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal/workfolder/SAC-X-DT/.conda/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/prajwal/workfolder/SAC-X-DT/.conda/lib/python3.10/site-packages/gym/envs/mujoco/mujoco_env.py:191: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n",
      "load datafile: 100%|██████████| 9/9 [00:00<00:00, 12.52it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset_name = \"hopper-random-v2\"\n",
    "test_dataset, env_info = get_dataset(test_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_obs = np.concatenate([test_dataset['infos/qpos'], test_dataset['infos/qvel']], axis=1)\n",
    "rand_act = test_dataset['actions']\n",
    "rand_next_obs = test_dataset['next_observations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_obs = np.concatenate([test_dataset['infos/qpos'], test_dataset['infos/qvel']], axis=1)\n",
    "rand_act = test_dataset['actions']\n",
    "rand_next_obs = test_dataset['next_observations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_obs_act = np.concatenate((rand_obs, rand_act), axis=1)\n",
    "rand_obs_act = normalize_data(rand_obs_act, stats['state_action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_rand_obs = rand_obs_act[:, :state_dim]\n",
    "normalized_rand_act = rand_obs_act[:, state_dim:]\n",
    "\n",
    "normalized_rand_act = normalized_rand_act.reshape(1000, 1000, -1)\n",
    "normalized_rand_obs = normalized_rand_obs.reshape(1000, 1000, -1)\n",
    "\n",
    "\n",
    "all_predicted_next_obs = list()\n",
    "for this_obs, this_act in zip(normalized_rand_obs, normalized_rand_act):\n",
    "    predicted_next_obs = predict_dynamics(\n",
    "                                model=ema_noise_pred_net,\n",
    "                                obs=this_obs,\n",
    "                                act=this_act,\n",
    "                                obs_horizon=1,\n",
    "                                device='cuda'\n",
    "                            )\n",
    "    all_predicted_next_obs.append(predicted_next_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(423, 1000, 12)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predicted_next_obs = np.array(all_predicted_next_obs)\n",
    "all_predicted_next_obs = unnormalize_data(all_predicted_next_obs, stats['next_state']).reshape(-1, state_dim)\n",
    "all_predicted_next_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
