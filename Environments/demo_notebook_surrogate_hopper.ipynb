{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "class Dynamics(nn.Module):\n",
    "    def __init__(self, state_dim, act_dim, model_size=[256, 256]):\n",
    "        super(Dynamics, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.model_size = model_size\n",
    "        self.fc1 = nn.Linear(state_dim + act_dim, model_size[0])\n",
    "        self.fc2 = nn.Linear(model_size[0], model_size[1])\n",
    "        self.fc3 = nn.Linear(model_size[1], state_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, obs, act):\n",
    "        x = torch.cat((obs, act), dim=1)\n",
    "        return self.forward(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class HopperSurrogateEnv(gym.Env):\n",
    "\n",
    "    def terminate(self, ob):\n",
    "        unhealthy = False\n",
    "        healthy_state = ob[:,1:]\n",
    "        height = ob[:,0]\n",
    "        angle = ob[:,1]\n",
    "        \n",
    "        if np.all((self.healthy_state_range[0] > healthy_state) + (self.healthy_state_range[1] < healthy_state)):\n",
    "            # print(\"terminating due to all state limits\")\n",
    "            unhealthy = True\n",
    "\n",
    "        \n",
    "        if np.all((self.healthy_z_range[0] > height) + (self.healthy_z_range[1] < height)):\n",
    "            # print(\"terminating due to height\")\n",
    "            unhealthy = True \n",
    "        \n",
    "        if np.all((self.healthy_angle_range[0] > angle) + (self.healthy_angle_range[1] < angle)):\n",
    "            # print(\"terminating due to angle\")\n",
    "            unhealthy = True  \n",
    "\n",
    "        return unhealthy\n",
    "\n",
    "    def __init__(self, render_mode=None, model_path=None):\n",
    "\n",
    "        self.observation_space = spaces.Box(low= float('-inf'), high= float('inf'), shape=(11,))\n",
    "        self.action_space = spaces.Box(-1, 1, (3,))\n",
    "\n",
    "\n",
    "        self.init_qpos = [0, 1.25, 0, 0, 0, 0]\n",
    "        self.init_qvel = [0, 0,    0, 0, 0, 0]\n",
    "        self.init_x = 0\n",
    "\n",
    "\n",
    "        self.forward_reward_weight=1.0\n",
    "        self.ctrl_cost_weight=1e-3\n",
    "        self.healthy_reward=1.0\n",
    "        self.terminate_when_unhealthy=True\n",
    "        self.healthy_state_range=(-100.0, 100.0)\n",
    "        self.healthy_z_range=(0.7, float(\"inf\"))\n",
    "        self.healthy_angle_range=(-0.2, 0.2)\n",
    "        self.reset_noise_scale=5e-3\n",
    "        self.exclude_current_positions_from_observation=True\n",
    "        self.timestep = 1\n",
    "\n",
    "        self.model = torch.load(model_path)\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.qpos = self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=int(len(self.init_qpos)))\n",
    "        self.qvel = self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=int(len(self.init_qvel)))\n",
    "        self.observation = np.concatenate([self.qpos, self.qvel]).reshape(-1, 12)\n",
    "        self.init_x = self.observation[:,0]\n",
    "        self.timestep = 1\n",
    "        return self.observation[:,1:], {\"full_state\":self.observation}\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        with torch.no_grad():\n",
    "            action = np.array(action).reshape(-1, 3)\n",
    "            obs = torch.tensor(self.observation, dtype=torch.float32)\n",
    "            act = torch.tensor(action, dtype=torch.float32)\n",
    "            next_obs = self.model.predict(obs, act)\n",
    "            next_obs = next_obs.detach().numpy()\n",
    "\n",
    "        final_x = next_obs[:,0]   \n",
    "        self.observation = next_obs\n",
    "\n",
    "        healthy_reward = self.healthy_reward\n",
    "        forward_reward = self.forward_reward_weight * (final_x - self.init_x ) / 0.008 \n",
    "        self.init_x = final_x\n",
    "        control_cost = self.ctrl_cost_weight * np.sum(action**2, axis=1)\n",
    "        reward = healthy_reward + forward_reward - control_cost\n",
    "\n",
    "        terminated = self.terminate(self.observation[:,1:])\n",
    "        \n",
    "        if self.timestep > 1000:\n",
    "            truncated = True\n",
    "        else:\n",
    "            truncated = False\n",
    "        \n",
    "        self.timestep += 1\n",
    "        self.init_x = final_x\n",
    "\n",
    "        return self.observation[:,1:], reward, terminated, truncated, {\"full_state\":self.observation}\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HopperSurrogateEnv(model_path='/home/prajwal/homework/me592/FinalProject/Deep-Learning-of-Robot-Dynamics/Experiments/Hopper/TrainedModels/vanilla_nn_dynamics_hopper.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.4628992 ,  0.19398974,  1.2791119 ,  0.96002185,  1.0163924 ,\n",
       "        0.75161767, -0.4426608 , -0.10419376, -1.2021351 ,  0.891419  ,\n",
       "       -0.8495554 ], dtype=float32)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13817625,  0.19347589, -0.3934131 ], dtype=float32)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode length: 47\n",
      "Total rewards: [-10.479429]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "done = False\n",
    "episode_length = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    episode_length += 1\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Episode length:\", episode_length)\n",
    "print(\"Total rewards:\", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
